
Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  [32m+[0m create[0m

Terraform will perform the following actions:

[1m  # module.telemetry.module.elasticsearch[0].kubernetes_config_map.elasticsearch[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_config_map" "elasticsearch" {
      [32m+[0m[0m data = {
          [32m+[0m[0m "elasticsearch.yml" = <<-EOT
                cluster.name: jaeger-cluster
                network.host: 0.0.0.0
                      
                # Minimum nodes required for cluster formation
                discovery.seed_hosts: elasticsearch-0.elasticsearch.telemetry.svc.cluster.local
                cluster.initial_master_nodes: elasticsearch-0
                      
                # Disable security features for internal use
                xpack.security.enabled: false
                xpack.monitoring.enabled: true
                      
                # Performance and memory settings
                indices.memory.index_buffer_size: 30%
                indices.queries.cache.size: 10%
                      
                # Index lifecycle management
                action.auto_create_index: true
            EOT
        }
      [32m+[0m[0m id   = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "elasticsearch"
              [32m+[0m[0m "component"   = "storage"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "elasticsearch-config"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }
    }

[1m  # module.telemetry.module.elasticsearch[0].kubernetes_cron_job_v1.index_cleanup[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_cron_job_v1" "index_cleanup" {
      [32m+[0m[0m id = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "elasticsearch"
              [32m+[0m[0m "component"   = "storage"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "elasticsearch-index-cleanup"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m concurrency_policy            = "Forbid"
          [32m+[0m[0m failed_jobs_history_limit     = 3
          [32m+[0m[0m schedule                      = "0 2 * * *"
          [32m+[0m[0m starting_deadline_seconds     = 0
          [32m+[0m[0m successful_jobs_history_limit = 3
          [32m+[0m[0m suspend                       = false

          [32m+[0m[0m job_template {
              [32m+[0m[0m metadata {
                  [32m+[0m[0m generation       = (known after apply)
                  [32m+[0m[0m labels           = {
                      [32m+[0m[0m "app"         = "elasticsearch"
                      [32m+[0m[0m "component"   = "storage"
                      [32m+[0m[0m "environment" = "staging"
                      [32m+[0m[0m "managed-by"  = "terraform"
                      [32m+[0m[0m "part-of"     = "telemetry"
                      [32m+[0m[0m "project"     = "telemetry"
                      [32m+[0m[0m "team"        = "platform"
                    }
                  [32m+[0m[0m name             = (known after apply)
                  [32m+[0m[0m resource_version = (known after apply)
                  [32m+[0m[0m uid              = (known after apply)
                }
              [32m+[0m[0m spec {
                  [32m+[0m[0m backoff_limit   = 6
                  [32m+[0m[0m completion_mode = (known after apply)
                  [32m+[0m[0m completions     = 1
                  [32m+[0m[0m parallelism     = 1

                  [32m+[0m[0m selector (known after apply)

                  [32m+[0m[0m template {
                      [32m+[0m[0m metadata {
                          [32m+[0m[0m generation       = (known after apply)
                          [32m+[0m[0m labels           = {
                              [32m+[0m[0m "app"         = "elasticsearch"
                              [32m+[0m[0m "component"   = "storage"
                              [32m+[0m[0m "environment" = "staging"
                              [32m+[0m[0m "managed-by"  = "terraform"
                              [32m+[0m[0m "part-of"     = "telemetry"
                              [32m+[0m[0m "project"     = "telemetry"
                              [32m+[0m[0m "team"        = "platform"
                            }
                          [32m+[0m[0m name             = (known after apply)
                          [32m+[0m[0m resource_version = (known after apply)
                          [32m+[0m[0m uid              = (known after apply)
                        }
                      [32m+[0m[0m spec {
                          [32m+[0m[0m automount_service_account_token  = true
                          [32m+[0m[0m dns_policy                       = "ClusterFirst"
                          [32m+[0m[0m enable_service_links             = true
                          [32m+[0m[0m host_ipc                         = false
                          [32m+[0m[0m host_network                     = false
                          [32m+[0m[0m host_pid                         = false
                          [32m+[0m[0m hostname                         = (known after apply)
                          [32m+[0m[0m node_name                        = (known after apply)
                          [32m+[0m[0m restart_policy                   = "OnFailure"
                          [32m+[0m[0m scheduler_name                   = (known after apply)
                          [32m+[0m[0m service_account_name             = (known after apply)
                          [32m+[0m[0m share_process_namespace          = false
                          [32m+[0m[0m termination_grace_period_seconds = 30

                          [32m+[0m[0m container {
                              [32m+[0m[0m command                    = [
                                  [32m+[0m[0m "/bin/sh",
                                  [32m+[0m[0m "-c",
                                  [32m+[0m[0m <<-EOT
                                        curator_cli \
                                          --host elasticsearch.telemetry.svc.cluster.local \
                                          --port 9200 \
                                          delete_indices \
                                          --filter_list '[{"filtertype":"age","source":"creation_date","direction":"older","unit":"days","unit_count":7},{"filtertype":"pattern","kind":"prefix","value":"jaeger"}]'
                                    EOT,
                                ]
                              [32m+[0m[0m image                      = "bitnami/elasticsearch-curator:5.8.4"
                              [32m+[0m[0m image_pull_policy          = (known after apply)
                              [32m+[0m[0m name                       = "curator"
                              [32m+[0m[0m stdin                      = false
                              [32m+[0m[0m stdin_once                 = false
                              [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                              [32m+[0m[0m termination_message_policy = (known after apply)
                              [32m+[0m[0m tty                        = false

                              [32m+[0m[0m resources (known after apply)
                            }

                          [32m+[0m[0m image_pull_secrets (known after apply)

                          [32m+[0m[0m readiness_gate (known after apply)
                        }
                    }
                }
            }
        }
    }

[1m  # module.telemetry.module.elasticsearch[0].kubernetes_pod_disruption_budget_v1.elasticsearch[0][0m will be created
[0m  [32m+[0m[0m resource "kubernetes_pod_disruption_budget_v1" "elasticsearch" {
      [32m+[0m[0m id = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "elasticsearch"
              [32m+[0m[0m "component"   = "storage"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "elasticsearch"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m min_available = "1"

          [32m+[0m[0m selector {
              [32m+[0m[0m match_labels = {
                  [32m+[0m[0m "app" = "elasticsearch"
                }
            }
        }
    }

[1m  # module.telemetry.module.elasticsearch[0].kubernetes_service.elasticsearch[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_service" "elasticsearch" {
      [32m+[0m[0m id                     = (known after apply)
      [32m+[0m[0m status                 = (known after apply)
      [32m+[0m[0m wait_for_load_balancer = true

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "elasticsearch"
              [32m+[0m[0m "component"   = "storage"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "elasticsearch"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m allocate_load_balancer_node_ports = true
          [32m+[0m[0m cluster_ip                        = "None"
          [32m+[0m[0m cluster_ips                       = (known after apply)
          [32m+[0m[0m external_traffic_policy           = (known after apply)
          [32m+[0m[0m health_check_node_port            = (known after apply)
          [32m+[0m[0m internal_traffic_policy           = (known after apply)
          [32m+[0m[0m ip_families                       = (known after apply)
          [32m+[0m[0m ip_family_policy                  = (known after apply)
          [32m+[0m[0m publish_not_ready_addresses       = false
          [32m+[0m[0m selector                          = {
              [32m+[0m[0m "app" = "elasticsearch"
            }
          [32m+[0m[0m session_affinity                  = "None"
          [32m+[0m[0m type                              = "ClusterIP"

          [32m+[0m[0m port {
              [32m+[0m[0m name        = "http"
              [32m+[0m[0m node_port   = (known after apply)
              [32m+[0m[0m port        = 9200
              [32m+[0m[0m protocol    = "TCP"
              [32m+[0m[0m target_port = "9200"
            }
          [32m+[0m[0m port {
              [32m+[0m[0m name        = "transport"
              [32m+[0m[0m node_port   = (known after apply)
              [32m+[0m[0m port        = 9300
              [32m+[0m[0m protocol    = "TCP"
              [32m+[0m[0m target_port = "9300"
            }

          [32m+[0m[0m session_affinity_config (known after apply)
        }
    }

[1m  # module.telemetry.module.elasticsearch[0].kubernetes_stateful_set.elasticsearch[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_stateful_set" "elasticsearch" {
      [32m+[0m[0m id               = (known after apply)
      [32m+[0m[0m wait_for_rollout = true

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "elasticsearch"
              [32m+[0m[0m "component"   = "storage"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "elasticsearch"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m min_ready_seconds      = 0
          [32m+[0m[0m pod_management_policy  = (known after apply)
          [32m+[0m[0m replicas               = "2"
          [32m+[0m[0m revision_history_limit = (known after apply)
          [32m+[0m[0m service_name           = "elasticsearch"

          [32m+[0m[0m persistent_volume_claim_retention_policy (known after apply)

          [32m+[0m[0m selector {
              [32m+[0m[0m match_labels = {
                  [32m+[0m[0m "app" = "elasticsearch"
                }
            }

          [32m+[0m[0m template {
              [32m+[0m[0m metadata {
                  [32m+[0m[0m generation       = (known after apply)
                  [32m+[0m[0m labels           = {
                      [32m+[0m[0m "app"         = "elasticsearch"
                      [32m+[0m[0m "component"   = "storage"
                      [32m+[0m[0m "environment" = "staging"
                      [32m+[0m[0m "managed-by"  = "terraform"
                      [32m+[0m[0m "part-of"     = "telemetry"
                      [32m+[0m[0m "project"     = "telemetry"
                      [32m+[0m[0m "team"        = "platform"
                    }
                  [32m+[0m[0m name             = (known after apply)
                  [32m+[0m[0m resource_version = (known after apply)
                  [32m+[0m[0m uid              = (known after apply)
                }
              [32m+[0m[0m spec {
                  [32m+[0m[0m automount_service_account_token  = true
                  [32m+[0m[0m dns_policy                       = "ClusterFirst"
                  [32m+[0m[0m enable_service_links             = true
                  [32m+[0m[0m host_ipc                         = false
                  [32m+[0m[0m host_network                     = false
                  [32m+[0m[0m host_pid                         = false
                  [32m+[0m[0m hostname                         = (known after apply)
                  [32m+[0m[0m node_name                        = (known after apply)
                  [32m+[0m[0m node_selector                    = {
                      [32m+[0m[0m "telemetry" = "true"
                    }
                  [32m+[0m[0m restart_policy                   = "Always"
                  [32m+[0m[0m scheduler_name                   = (known after apply)
                  [32m+[0m[0m service_account_name             = (known after apply)
                  [32m+[0m[0m share_process_namespace          = false
                  [32m+[0m[0m termination_grace_period_seconds = 30

                  [32m+[0m[0m affinity {
                      [32m+[0m[0m pod_anti_affinity {
                          [32m+[0m[0m preferred_during_scheduling_ignored_during_execution {
                              [32m+[0m[0m weight = 100

                              [32m+[0m[0m pod_affinity_term {
                                  [32m+[0m[0m topology_key = "kubernetes.io/hostname"

                                  [32m+[0m[0m label_selector {
                                      [32m+[0m[0m match_expressions {
                                          [32m+[0m[0m key      = "app"
                                          [32m+[0m[0m operator = "In"
                                          [32m+[0m[0m values   = [
                                              [32m+[0m[0m "elasticsearch",
                                            ]
                                        }
                                    }
                                }
                            }
                        }
                    }

                  [32m+[0m[0m container {
                      [32m+[0m[0m image                      = "docker.elastic.co/elasticsearch/elasticsearch:8.12.0"
                      [32m+[0m[0m image_pull_policy          = (known after apply)
                      [32m+[0m[0m name                       = "elasticsearch"
                      [32m+[0m[0m stdin                      = false
                      [32m+[0m[0m stdin_once                 = false
                      [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                      [32m+[0m[0m termination_message_policy = (known after apply)
                      [32m+[0m[0m tty                        = false

                      [32m+[0m[0m env {
                          [32m+[0m[0m name  = "cluster.name"
                          [32m+[0m[0m value = "jaeger-cluster"
                        }
                      [32m+[0m[0m env {
                          [32m+[0m[0m name = "node.name"

                          [32m+[0m[0m value_from {
                              [32m+[0m[0m field_ref {
                                  [32m+[0m[0m api_version = "v1"
                                  [32m+[0m[0m field_path  = "metadata.name"
                                }
                            }
                        }
                      [32m+[0m[0m env {
                          [32m+[0m[0m name  = "discovery.type"
                            [90m# (1 unchanged attribute hidden)[0m[0m
                        }
                      [32m+[0m[0m env {
                          [32m+[0m[0m name  = "ES_JAVA_OPTS"
                          [32m+[0m[0m value = "-Xms2Gi -Xmx4Gi"
                        }
                      [32m+[0m[0m env {
                          [32m+[0m[0m name  = "xpack.security.enabled"
                          [32m+[0m[0m value = "false"
                        }

                      [32m+[0m[0m liveness_probe {
                          [32m+[0m[0m failure_threshold     = 3
                          [32m+[0m[0m initial_delay_seconds = 90
                          [32m+[0m[0m period_seconds        = 30
                          [32m+[0m[0m success_threshold     = 1
                          [32m+[0m[0m timeout_seconds       = 10

                          [32m+[0m[0m http_get {
                              [32m+[0m[0m path   = "/_cluster/health?local=true"
                              [32m+[0m[0m port   = "9200"
                              [32m+[0m[0m scheme = "HTTP"
                            }
                        }

                      [32m+[0m[0m port {
                          [32m+[0m[0m container_port = 9200
                          [32m+[0m[0m name           = "http"
                          [32m+[0m[0m protocol       = "TCP"
                        }
                      [32m+[0m[0m port {
                          [32m+[0m[0m container_port = 9300
                          [32m+[0m[0m name           = "transport"
                          [32m+[0m[0m protocol       = "TCP"
                        }

                      [32m+[0m[0m readiness_probe {
                          [32m+[0m[0m failure_threshold     = 3
                          [32m+[0m[0m initial_delay_seconds = 60
                          [32m+[0m[0m period_seconds        = 10
                          [32m+[0m[0m success_threshold     = 1
                          [32m+[0m[0m timeout_seconds       = 5

                          [32m+[0m[0m http_get {
                              [32m+[0m[0m path   = "/_cluster/health?local=true"
                              [32m+[0m[0m port   = "9200"
                              [32m+[0m[0m scheme = "HTTP"
                            }
                        }

                      [32m+[0m[0m resources {
                          [32m+[0m[0m limits   = {
                              [32m+[0m[0m "cpu"    = "2000m"
                              [32m+[0m[0m "memory" = "4Gi"
                            }
                          [32m+[0m[0m requests = {
                              [32m+[0m[0m "cpu"    = "1000m"
                              [32m+[0m[0m "memory" = "2Gi"
                            }
                        }

                      [32m+[0m[0m volume_mount {
                          [32m+[0m[0m mount_path        = "/usr/share/elasticsearch/data"
                          [32m+[0m[0m mount_propagation = "None"
                          [32m+[0m[0m name              = "data"
                          [32m+[0m[0m read_only         = false
                        }
                      [32m+[0m[0m volume_mount {
                          [32m+[0m[0m mount_path        = "/usr/share/elasticsearch/config/elasticsearch.yml"
                          [32m+[0m[0m mount_propagation = "None"
                          [32m+[0m[0m name              = "config"
                          [32m+[0m[0m read_only         = true
                          [32m+[0m[0m sub_path          = "elasticsearch.yml"
                        }
                    }

                  [32m+[0m[0m image_pull_secrets (known after apply)

                  [32m+[0m[0m init_container {
                      [32m+[0m[0m command                    = [
                          [32m+[0m[0m "sh",
                          [32m+[0m[0m "-c",
                          [32m+[0m[0m "chown -R 1000:1000 /usr/share/elasticsearch/data",
                        ]
                      [32m+[0m[0m image                      = "busybox:1.36"
                      [32m+[0m[0m image_pull_policy          = (known after apply)
                      [32m+[0m[0m name                       = "fix-permissions"
                      [32m+[0m[0m stdin                      = false
                      [32m+[0m[0m stdin_once                 = false
                      [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                      [32m+[0m[0m termination_message_policy = (known after apply)
                      [32m+[0m[0m tty                        = false

                      [32m+[0m[0m resources (known after apply)

                      [32m+[0m[0m security_context {
                          [32m+[0m[0m allow_privilege_escalation = true
                          [32m+[0m[0m privileged                 = false
                          [32m+[0m[0m read_only_root_filesystem  = false
                          [32m+[0m[0m run_as_user                = "0"
                        }

                      [32m+[0m[0m volume_mount {
                          [32m+[0m[0m mount_path        = "/usr/share/elasticsearch/data"
                          [32m+[0m[0m mount_propagation = "None"
                          [32m+[0m[0m name              = "data"
                          [32m+[0m[0m read_only         = false
                        }
                    }
                  [32m+[0m[0m init_container {
                      [32m+[0m[0m command                    = [
                          [32m+[0m[0m "sysctl",
                          [32m+[0m[0m "-w",
                          [32m+[0m[0m "vm.max_map_count=262144",
                        ]
                      [32m+[0m[0m image                      = "busybox:1.36"
                      [32m+[0m[0m image_pull_policy          = (known after apply)
                      [32m+[0m[0m name                       = "increase-vm-max-map"
                      [32m+[0m[0m stdin                      = false
                      [32m+[0m[0m stdin_once                 = false
                      [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                      [32m+[0m[0m termination_message_policy = (known after apply)
                      [32m+[0m[0m tty                        = false

                      [32m+[0m[0m resources (known after apply)

                      [32m+[0m[0m security_context {
                          [32m+[0m[0m allow_privilege_escalation = true
                          [32m+[0m[0m privileged                 = true
                          [32m+[0m[0m read_only_root_filesystem  = false
                        }
                    }

                  [32m+[0m[0m readiness_gate (known after apply)

                  [32m+[0m[0m volume {
                      [32m+[0m[0m name = "config"

                      [32m+[0m[0m config_map {
                          [32m+[0m[0m default_mode = "0644"
                          [32m+[0m[0m name         = "elasticsearch-config"
                        }
                    }
                }
            }

          [32m+[0m[0m update_strategy {
              [32m+[0m[0m type = "RollingUpdate"

              [32m+[0m[0m rolling_update {
                  [32m+[0m[0m partition = 0
                }
            }

          [32m+[0m[0m volume_claim_template {
              [32m+[0m[0m metadata {
                  [32m+[0m[0m generation       = (known after apply)
                  [32m+[0m[0m labels           = {
                      [32m+[0m[0m "app"         = "elasticsearch"
                      [32m+[0m[0m "component"   = "storage"
                      [32m+[0m[0m "environment" = "staging"
                      [32m+[0m[0m "managed-by"  = "terraform"
                      [32m+[0m[0m "part-of"     = "telemetry"
                      [32m+[0m[0m "project"     = "telemetry"
                      [32m+[0m[0m "team"        = "platform"
                    }
                  [32m+[0m[0m name             = "data"
                  [32m+[0m[0m namespace        = "default"
                  [32m+[0m[0m resource_version = (known after apply)
                  [32m+[0m[0m uid              = (known after apply)
                }
              [32m+[0m[0m spec {
                  [32m+[0m[0m access_modes       = [
                      [32m+[0m[0m "ReadWriteOnce",
                    ]
                  [32m+[0m[0m storage_class_name = "gp3"
                  [32m+[0m[0m volume_mode        = (known after apply)
                  [32m+[0m[0m volume_name        = (known after apply)

                  [32m+[0m[0m resources {
                      [32m+[0m[0m requests = {
                          [32m+[0m[0m "storage" = "75Gi"
                        }
                    }
                }
            }
        }
    }

[1m  # module.telemetry.module.jaeger.helm_release.jaeger[0m will be created
[0m  [32m+[0m[0m resource "helm_release" "jaeger" {
      [32m+[0m[0m atomic                     = false
      [32m+[0m[0m chart                      = "jaeger"
      [32m+[0m[0m cleanup_on_fail            = false
      [32m+[0m[0m create_namespace           = false
      [32m+[0m[0m dependency_update          = false
      [32m+[0m[0m disable_crd_hooks          = false
      [32m+[0m[0m disable_openapi_validation = false
      [32m+[0m[0m disable_webhooks           = false
      [32m+[0m[0m force_update               = false
      [32m+[0m[0m id                         = (known after apply)
      [32m+[0m[0m lint                       = false
      [32m+[0m[0m manifest                   = (known after apply)
      [32m+[0m[0m max_history                = 0
      [32m+[0m[0m metadata                   = (known after apply)
      [32m+[0m[0m name                       = "jaeger"
      [32m+[0m[0m namespace                  = "telemetry"
      [32m+[0m[0m pass_credentials           = false
      [32m+[0m[0m recreate_pods              = false
      [32m+[0m[0m render_subchart_notes      = true
      [32m+[0m[0m replace                    = false
      [32m+[0m[0m repository                 = "https://jaegertracing.github.io/helm-charts"
      [32m+[0m[0m reset_values               = false
      [32m+[0m[0m reuse_values               = false
      [32m+[0m[0m skip_crds                  = false
      [32m+[0m[0m status                     = "deployed"
      [32m+[0m[0m timeout                    = 600
      [32m+[0m[0m values                     = [
          [32m+[0m[0m <<-EOT
                "agent":
                  "enabled": false
                "allInOne":
                  "enabled": false
                "collector":
                  "affinity":
                    "podAntiAffinity":
                      "preferredDuringSchedulingIgnoredDuringExecution":
                      - "podAffinityTerm":
                          "labelSelector":
                            "matchExpressions":
                            - "key": "app.kubernetes.io/component"
                              "operator": "In"
                              "values":
                              - "collector"
                          "topologyKey": "kubernetes.io/hostname"
                        "weight": 100
                  "autoscaling":
                    "enabled": false
                  "enabled": true
                  "podDisruptionBudget":
                    "enabled": true
                    "minAvailable": 1
                  "replicaCount": 2
                  "resources":
                    "limits":
                      "cpu": "500m"
                      "memory": "512Mi"
                    "requests":
                      "cpu": "100m"
                      "memory": "256Mi"
                  "service":
                    "grpc":
                      "port": 14250
                    "http":
                      "port": 14268
                    "otlp":
                      "grpc":
                        "port": 4317
                      "http":
                        "port": 4318
                    "type": "ClusterIP"
                  "strategy":
                    "rollingUpdate":
                      "maxSurge": "25%"
                      "maxUnavailable": "25%"
                    "type": "RollingUpdate"
                "ingress":
                  "enabled": false
                "provisionDataStore":
                  "cassandra": false
                  "elasticsearch": true
                "query":
                  "affinity":
                    "podAntiAffinity":
                      "preferredDuringSchedulingIgnoredDuringExecution":
                      - "podAffinityTerm":
                          "labelSelector":
                            "matchExpressions":
                            - "key": "app.kubernetes.io/component"
                              "operator": "In"
                              "values":
                              - "query"
                          "topologyKey": "kubernetes.io/hostname"
                        "weight": 100
                  "enabled": true
                  "podDisruptionBudget":
                    "enabled": true
                    "minAvailable": 1
                  "replicaCount": 2
                  "resources":
                    "limits":
                      "cpu": "500m"
                      "memory": "512Mi"
                    "requests":
                      "cpu": "100m"
                      "memory": "128Mi"
                  "service":
                    "port": 16686
                    "type": "ClusterIP"
                  "strategy":
                    "rollingUpdate":
                      "maxSurge": "25%"
                      "maxUnavailable": "25%"
                    "type": "RollingUpdate"
                "storage":
                  "elasticsearch":
                    "host": "elasticsearch.telemetry.svc.cluster.local"
                    "password": ""
                    "port": 9200
                    "scheme": "http"
                    "user": ""
                  "type": "elasticsearch"
            EOT,
        ]
      [32m+[0m[0m verify                     = false
      [32m+[0m[0m version                    = "2.0.0"
      [32m+[0m[0m wait                       = true
      [32m+[0m[0m wait_for_jobs              = false

      [32m+[0m[0m set {
          [32m+[0m[0m name  = "commonLabels.environment"
          [32m+[0m[0m value = "staging"
            [90m# (1 unchanged attribute hidden)[0m[0m
        }
      [32m+[0m[0m set {
          [32m+[0m[0m name  = "commonLabels.managed-by"
          [32m+[0m[0m value = "terraform"
            [90m# (1 unchanged attribute hidden)[0m[0m
        }
    }

[1m  # module.telemetry.module.namespace.kubernetes_namespace.this[0][0m will be created
[0m  [32m+[0m[0m resource "kubernetes_namespace" "this" {
      [32m+[0m[0m id                               = (known after apply)
      [32m+[0m[0m wait_for_default_service_account = false

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }
    }

[1m  # module.telemetry.module.otel_collector.kubernetes_cluster_role.otel_collector[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_cluster_role" "otel_collector" {
      [32m+[0m[0m id = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "otel-collector"
              [32m+[0m[0m "component"   = "collector"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "otel-collector-telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m rule {
          [32m+[0m[0m api_groups = [
              [32m+[0m[0m [90mnull[0m[0m,
            ]
          [32m+[0m[0m resources  = [
              [32m+[0m[0m "pods",
              [32m+[0m[0m "namespaces",
              [32m+[0m[0m "nodes",
            ]
          [32m+[0m[0m verbs      = [
              [32m+[0m[0m "get",
              [32m+[0m[0m "watch",
              [32m+[0m[0m "list",
            ]
        }
      [32m+[0m[0m rule {
          [32m+[0m[0m api_groups = [
              [32m+[0m[0m "apps",
            ]
          [32m+[0m[0m resources  = [
              [32m+[0m[0m "replicasets",
              [32m+[0m[0m "deployments",
            ]
          [32m+[0m[0m verbs      = [
              [32m+[0m[0m "get",
              [32m+[0m[0m "list",
              [32m+[0m[0m "watch",
            ]
        }
    }

[1m  # module.telemetry.module.otel_collector.kubernetes_cluster_role_binding.otel_collector[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_cluster_role_binding" "otel_collector" {
      [32m+[0m[0m id = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "otel-collector"
              [32m+[0m[0m "component"   = "collector"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "otel-collector-telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m role_ref {
          [32m+[0m[0m api_group = "rbac.authorization.k8s.io"
          [32m+[0m[0m kind      = "ClusterRole"
          [32m+[0m[0m name      = "otel-collector-telemetry"
        }

      [32m+[0m[0m subject {
          [32m+[0m[0m api_group = (known after apply)
          [32m+[0m[0m kind      = "ServiceAccount"
          [32m+[0m[0m name      = "otel-collector"
          [32m+[0m[0m namespace = "telemetry"
        }
    }

[1m  # module.telemetry.module.otel_collector.kubernetes_config_map.otel_collector[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_config_map" "otel_collector" {
      [32m+[0m[0m data = {
          [32m+[0m[0m "otel-collector-config.yaml" = <<-EOT
                "exporters":
                  "jaeger":
                    "endpoint": "jaeger-collector.telemetry.svc.cluster.local:14250"
                    "tls":
                      "insecure": true
                  "logging":
                    "loglevel": "info"
                    "sampling_initial": 5
                    "sampling_thereafter": 200
                  "otlp":
                    "endpoint": "jaeger-collector.telemetry.svc.cluster.local:14250"
                    "tls":
                      "insecure": true
                "extensions":
                  "health_check":
                    "endpoint": "0.0.0.0:13133"
                  "pprof":
                    "endpoint": "0.0.0.0:1777"
                  "zpages":
                    "endpoint": "0.0.0.0:55679"
                "processors":
                  "batch":
                    "send_batch_max_size": 2048
                    "send_batch_size": 1024
                    "timeout": "10s"
                  "k8sattributes":
                    "auth_type": "serviceAccount"
                    "extract":
                      "labels":
                      - "from": "pod"
                        "key": "app"
                        "tag_name": "app"
                      "metadata":
                      - "k8s.namespace.name"
                      - "k8s.deployment.name"
                      - "k8s.pod.name"
                      - "k8s.pod.uid"
                      - "k8s.node.name"
                    "passthrough": false
                  "memory_limiter":
                    "check_interval": "1s"
                    "limit_mib": 512
                    "limit_percentage": 80
                    "spike_limit_mib": 128
                    "spike_limit_percentage": 20
                  "resource":
                    "attributes":
                    - "action": "insert"
                      "key": "environment"
                      "value": "staging"
                    - "action": "insert"
                      "key": "k8s.cluster.name"
                      "value": "default"
                "receivers":
                  "otlp":
                    "protocols":
                      "grpc":
                        "endpoint": "0.0.0.0:4317"
                      "http":
                        "endpoint": "0.0.0.0:4318"
                  "prometheus":
                    "config":
                      "scrape_configs":
                      - "job_name": "otel-collector"
                        "scrape_interval": "30s"
                        "static_configs":
                        - "targets":
                          - "localhost:8888"
                "service":
                  "extensions":
                  - "health_check"
                  - "pprof"
                  - "zpages"
                  "pipelines":
                    "metrics":
                      "exporters":
                      - "logging"
                      "processors":
                      - "memory_limiter"
                      - "batch"
                      "receivers":
                      - "otlp"
                      - "prometheus"
                    "traces":
                      "exporters":
                      - "jaeger"
                      - "logging"
                      "processors":
                      - "memory_limiter"
                      - "k8sattributes"
                      - "resource"
                      - "batch"
                      "receivers":
                      - "otlp"
                  "telemetry":
                    "logs":
                      "level": "info"
                    "metrics":
                      "address": "0.0.0.0:8888"
                      "level": "detailed"
            EOT
        }
      [32m+[0m[0m id   = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "otel-collector"
              [32m+[0m[0m "component"   = "collector"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "otel-collector-config"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }
    }

[1m  # module.telemetry.module.otel_collector.kubernetes_deployment.otel_collector[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_deployment" "otel_collector" {
      [32m+[0m[0m id               = (known after apply)
      [32m+[0m[0m wait_for_rollout = true

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "otel-collector"
              [32m+[0m[0m "component"   = "collector"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "otel-collector"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m min_ready_seconds         = 0
          [32m+[0m[0m paused                    = false
          [32m+[0m[0m progress_deadline_seconds = 600
          [32m+[0m[0m replicas                  = (known after apply)
          [32m+[0m[0m revision_history_limit    = 10

          [32m+[0m[0m selector {
              [32m+[0m[0m match_labels = {
                  [32m+[0m[0m "app" = "otel-collector"
                }
            }

          [32m+[0m[0m strategy {
              [32m+[0m[0m type = "RollingUpdate"

              [32m+[0m[0m rolling_update {
                  [32m+[0m[0m max_surge       = "25%"
                  [32m+[0m[0m max_unavailable = "25%"
                }
            }

          [32m+[0m[0m template {
              [32m+[0m[0m metadata {
                  [32m+[0m[0m annotations      = {
                      [32m+[0m[0m "prometheus.io/path"   = "/metrics"
                      [32m+[0m[0m "prometheus.io/port"   = "8888"
                      [32m+[0m[0m "prometheus.io/scrape" = "true"
                    }
                  [32m+[0m[0m generation       = (known after apply)
                  [32m+[0m[0m labels           = {
                      [32m+[0m[0m "app"         = "otel-collector"
                      [32m+[0m[0m "component"   = "collector"
                      [32m+[0m[0m "environment" = "staging"
                      [32m+[0m[0m "managed-by"  = "terraform"
                      [32m+[0m[0m "part-of"     = "telemetry"
                      [32m+[0m[0m "project"     = "telemetry"
                      [32m+[0m[0m "team"        = "platform"
                    }
                  [32m+[0m[0m name             = (known after apply)
                  [32m+[0m[0m resource_version = (known after apply)
                  [32m+[0m[0m uid              = (known after apply)
                }
              [32m+[0m[0m spec {
                  [32m+[0m[0m automount_service_account_token  = true
                  [32m+[0m[0m dns_policy                       = "ClusterFirst"
                  [32m+[0m[0m enable_service_links             = true
                  [32m+[0m[0m host_ipc                         = false
                  [32m+[0m[0m host_network                     = false
                  [32m+[0m[0m host_pid                         = false
                  [32m+[0m[0m hostname                         = (known after apply)
                  [32m+[0m[0m node_name                        = (known after apply)
                  [32m+[0m[0m node_selector                    = {
                      [32m+[0m[0m "telemetry" = "true"
                    }
                  [32m+[0m[0m restart_policy                   = "Always"
                  [32m+[0m[0m scheduler_name                   = (known after apply)
                  [32m+[0m[0m service_account_name             = "otel-collector"
                  [32m+[0m[0m share_process_namespace          = false
                  [32m+[0m[0m termination_grace_period_seconds = 30

                  [32m+[0m[0m affinity {
                      [32m+[0m[0m pod_anti_affinity {
                          [32m+[0m[0m preferred_during_scheduling_ignored_during_execution {
                              [32m+[0m[0m weight = 100

                              [32m+[0m[0m pod_affinity_term {
                                  [32m+[0m[0m topology_key = "kubernetes.io/hostname"

                                  [32m+[0m[0m label_selector {
                                      [32m+[0m[0m match_expressions {
                                          [32m+[0m[0m key      = "app"
                                          [32m+[0m[0m operator = "In"
                                          [32m+[0m[0m values   = [
                                              [32m+[0m[0m "otel-collector",
                                            ]
                                        }
                                    }
                                }
                            }
                        }
                    }

                  [32m+[0m[0m container {
                      [32m+[0m[0m args                       = [
                          [32m+[0m[0m "--config=/conf/otel-collector-config.yaml",
                        ]
                      [32m+[0m[0m image                      = "otel/opentelemetry-collector-contrib:0.95.0"
                      [32m+[0m[0m image_pull_policy          = (known after apply)
                      [32m+[0m[0m name                       = "otel-collector"
                      [32m+[0m[0m stdin                      = false
                      [32m+[0m[0m stdin_once                 = false
                      [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                      [32m+[0m[0m termination_message_policy = (known after apply)
                      [32m+[0m[0m tty                        = false

                      [32m+[0m[0m env {
                          [32m+[0m[0m name = "K8S_NODE_NAME"

                          [32m+[0m[0m value_from {
                              [32m+[0m[0m field_ref {
                                  [32m+[0m[0m api_version = "v1"
                                  [32m+[0m[0m field_path  = "spec.nodeName"
                                }
                            }
                        }
                      [32m+[0m[0m env {
                          [32m+[0m[0m name = "K8S_POD_NAME"

                          [32m+[0m[0m value_from {
                              [32m+[0m[0m field_ref {
                                  [32m+[0m[0m api_version = "v1"
                                  [32m+[0m[0m field_path  = "metadata.name"
                                }
                            }
                        }
                      [32m+[0m[0m env {
                          [32m+[0m[0m name = "K8S_POD_NAMESPACE"

                          [32m+[0m[0m value_from {
                              [32m+[0m[0m field_ref {
                                  [32m+[0m[0m api_version = "v1"
                                  [32m+[0m[0m field_path  = "metadata.namespace"
                                }
                            }
                        }
                      [32m+[0m[0m env {
                          [32m+[0m[0m name = "K8S_POD_IP"

                          [32m+[0m[0m value_from {
                              [32m+[0m[0m field_ref {
                                  [32m+[0m[0m api_version = "v1"
                                  [32m+[0m[0m field_path  = "status.podIP"
                                }
                            }
                        }

                      [32m+[0m[0m liveness_probe {
                          [32m+[0m[0m failure_threshold     = 3
                          [32m+[0m[0m initial_delay_seconds = 15
                          [32m+[0m[0m period_seconds        = 20
                          [32m+[0m[0m success_threshold     = 1
                          [32m+[0m[0m timeout_seconds       = 5

                          [32m+[0m[0m http_get {
                              [32m+[0m[0m path   = "/"
                              [32m+[0m[0m port   = "13133"
                              [32m+[0m[0m scheme = "HTTP"
                            }
                        }

                      [32m+[0m[0m port {
                          [32m+[0m[0m container_port = 4317
                          [32m+[0m[0m name           = "otlp-grpc"
                          [32m+[0m[0m protocol       = "TCP"
                        }
                      [32m+[0m[0m port {
                          [32m+[0m[0m container_port = 4318
                          [32m+[0m[0m name           = "otlp-http"
                          [32m+[0m[0m protocol       = "TCP"
                        }
                      [32m+[0m[0m port {
                          [32m+[0m[0m container_port = 8888
                          [32m+[0m[0m name           = "metrics"
                          [32m+[0m[0m protocol       = "TCP"
                        }
                      [32m+[0m[0m port {
                          [32m+[0m[0m container_port = 13133
                          [32m+[0m[0m name           = "health"
                          [32m+[0m[0m protocol       = "TCP"
                        }
                      [32m+[0m[0m port {
                          [32m+[0m[0m container_port = 55679
                          [32m+[0m[0m name           = "zpages"
                          [32m+[0m[0m protocol       = "TCP"
                        }

                      [32m+[0m[0m readiness_probe {
                          [32m+[0m[0m failure_threshold     = 3
                          [32m+[0m[0m initial_delay_seconds = 10
                          [32m+[0m[0m period_seconds        = 10
                          [32m+[0m[0m success_threshold     = 1
                          [32m+[0m[0m timeout_seconds       = 5

                          [32m+[0m[0m http_get {
                              [32m+[0m[0m path   = "/"
                              [32m+[0m[0m port   = "13133"
                              [32m+[0m[0m scheme = "HTTP"
                            }
                        }

                      [32m+[0m[0m resources {
                          [32m+[0m[0m limits   = {
                              [32m+[0m[0m "cpu"    = "1500m"
                              [32m+[0m[0m "memory" = "2Gi"
                            }
                          [32m+[0m[0m requests = {
                              [32m+[0m[0m "cpu"    = "300m"
                              [32m+[0m[0m "memory" = "512Mi"
                            }
                        }

                      [32m+[0m[0m volume_mount {
                          [32m+[0m[0m mount_path        = "/conf"
                          [32m+[0m[0m mount_propagation = "None"
                          [32m+[0m[0m name              = "otel-collector-config"
                          [32m+[0m[0m read_only         = false
                        }
                    }

                  [32m+[0m[0m image_pull_secrets (known after apply)

                  [32m+[0m[0m readiness_gate (known after apply)

                  [32m+[0m[0m volume {
                      [32m+[0m[0m name = "otel-collector-config"

                      [32m+[0m[0m config_map {
                          [32m+[0m[0m default_mode = "0644"
                          [32m+[0m[0m name         = "otel-collector-config"
                        }
                    }
                }
            }
        }
    }

[1m  # module.telemetry.module.otel_collector.kubernetes_horizontal_pod_autoscaler_v2.otel_collector[0][0m will be created
[0m  [32m+[0m[0m resource "kubernetes_horizontal_pod_autoscaler_v2" "otel_collector" {
      [32m+[0m[0m id = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "otel-collector"
              [32m+[0m[0m "component"   = "collector"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "otel-collector-hpa"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m max_replicas                      = 8
          [32m+[0m[0m min_replicas                      = 2
          [32m+[0m[0m target_cpu_utilization_percentage = (known after apply)

          [32m+[0m[0m behavior {
              [32m+[0m[0m scale_down {
                  [32m+[0m[0m stabilization_window_seconds = 300

                  [32m+[0m[0m policy {
                      [32m+[0m[0m period_seconds = 60
                      [32m+[0m[0m type           = "Percent"
                      [32m+[0m[0m value          = 50
                    }
                }
              [32m+[0m[0m scale_up {
                  [32m+[0m[0m select_policy                = "Max"
                  [32m+[0m[0m stabilization_window_seconds = 60

                  [32m+[0m[0m policy {
                      [32m+[0m[0m period_seconds = 60
                      [32m+[0m[0m type           = "Percent"
                      [32m+[0m[0m value          = 100
                    }
                  [32m+[0m[0m policy {
                      [32m+[0m[0m period_seconds = 60
                      [32m+[0m[0m type           = "Pods"
                      [32m+[0m[0m value          = 2
                    }
                }
            }

          [32m+[0m[0m metric {
              [32m+[0m[0m type = "Resource"

              [32m+[0m[0m resource {
                  [32m+[0m[0m name = "cpu"

                  [32m+[0m[0m target {
                      [32m+[0m[0m average_utilization = 70
                      [32m+[0m[0m type                = "Utilization"
                    }
                }
            }
          [32m+[0m[0m metric {
              [32m+[0m[0m type = "Resource"

              [32m+[0m[0m resource {
                  [32m+[0m[0m name = "memory"

                  [32m+[0m[0m target {
                      [32m+[0m[0m average_utilization = 80
                      [32m+[0m[0m type                = "Utilization"
                    }
                }
            }

          [32m+[0m[0m scale_target_ref {
              [32m+[0m[0m api_version = "apps/v1"
              [32m+[0m[0m kind        = "Deployment"
              [32m+[0m[0m name        = "otel-collector"
            }
        }
    }

[1m  # module.telemetry.module.otel_collector.kubernetes_pod_disruption_budget_v1.otel_collector[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_pod_disruption_budget_v1" "otel_collector" {
      [32m+[0m[0m id = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "otel-collector"
              [32m+[0m[0m "component"   = "collector"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "otel-collector"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m min_available = "50%"

          [32m+[0m[0m selector {
              [32m+[0m[0m match_labels = {
                  [32m+[0m[0m "app" = "otel-collector"
                }
            }
        }
    }

[1m  # module.telemetry.module.otel_collector.kubernetes_service.otel_collector[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_service" "otel_collector" {
      [32m+[0m[0m id                     = (known after apply)
      [32m+[0m[0m status                 = (known after apply)
      [32m+[0m[0m wait_for_load_balancer = true

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "otel-collector"
              [32m+[0m[0m "component"   = "collector"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "otel-collector"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m allocate_load_balancer_node_ports = true
          [32m+[0m[0m cluster_ip                        = (known after apply)
          [32m+[0m[0m cluster_ips                       = (known after apply)
          [32m+[0m[0m external_traffic_policy           = (known after apply)
          [32m+[0m[0m health_check_node_port            = (known after apply)
          [32m+[0m[0m internal_traffic_policy           = (known after apply)
          [32m+[0m[0m ip_families                       = (known after apply)
          [32m+[0m[0m ip_family_policy                  = (known after apply)
          [32m+[0m[0m publish_not_ready_addresses       = false
          [32m+[0m[0m selector                          = {
              [32m+[0m[0m "app" = "otel-collector"
            }
          [32m+[0m[0m session_affinity                  = "None"
          [32m+[0m[0m type                              = "ClusterIP"

          [32m+[0m[0m port {
              [32m+[0m[0m name        = "otlp-grpc"
              [32m+[0m[0m node_port   = (known after apply)
              [32m+[0m[0m port        = 4317
              [32m+[0m[0m protocol    = "TCP"
              [32m+[0m[0m target_port = "4317"
            }
          [32m+[0m[0m port {
              [32m+[0m[0m name        = "otlp-http"
              [32m+[0m[0m node_port   = (known after apply)
              [32m+[0m[0m port        = 4318
              [32m+[0m[0m protocol    = "TCP"
              [32m+[0m[0m target_port = "4318"
            }
          [32m+[0m[0m port {
              [32m+[0m[0m name        = "metrics"
              [32m+[0m[0m node_port   = (known after apply)
              [32m+[0m[0m port        = 8888
              [32m+[0m[0m protocol    = "TCP"
              [32m+[0m[0m target_port = "8888"
            }
          [32m+[0m[0m port {
              [32m+[0m[0m name        = "health"
              [32m+[0m[0m node_port   = (known after apply)
              [32m+[0m[0m port        = 13133
              [32m+[0m[0m protocol    = "TCP"
              [32m+[0m[0m target_port = "13133"
            }
          [32m+[0m[0m port {
              [32m+[0m[0m name        = "zpages"
              [32m+[0m[0m node_port   = (known after apply)
              [32m+[0m[0m port        = 55679
              [32m+[0m[0m protocol    = "TCP"
              [32m+[0m[0m target_port = "55679"
            }

          [32m+[0m[0m session_affinity_config (known after apply)
        }
    }

[1m  # module.telemetry.module.otel_collector.kubernetes_service_account.otel_collector[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_service_account" "otel_collector" {
      [32m+[0m[0m automount_service_account_token = true
      [32m+[0m[0m default_secret_name             = (known after apply)
      [32m+[0m[0m id                              = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app"         = "otel-collector"
              [32m+[0m[0m "component"   = "collector"
              [32m+[0m[0m "environment" = "staging"
              [32m+[0m[0m "managed-by"  = "terraform"
              [32m+[0m[0m "part-of"     = "telemetry"
              [32m+[0m[0m "project"     = "telemetry"
              [32m+[0m[0m "team"        = "platform"
            }
          [32m+[0m[0m name             = "otel-collector"
          [32m+[0m[0m namespace        = "telemetry"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }
    }

[1mPlan:[0m 15 to add, 0 to change, 0 to destroy.
[0m